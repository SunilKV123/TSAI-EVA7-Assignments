{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "dad7bd3c2fe54d1945d9dd1e1f6e10c6ddc91253bda8e79c84d666b43403f32c"
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit ('swaroop': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "mnist_add.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hicNtTH1fYRF"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S5BZcLGemgM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMrgg8ZauZ7B"
      },
      "source": [
        "#**Model**\n",
        "### Here we create a sequential network of convolution and linear layers to predict the mnist digit & the sum of predicted digit & random number\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggGs1sLNee7v"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        #layer1\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(1, 16, 3, padding=1), \n",
        "                                   nn.BatchNorm2d(16),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(0.1),\n",
        "                                   nn.Conv2d(16, 16, 3, padding=1),\n",
        "                                   nn.BatchNorm2d(16),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(0.1),\n",
        "                                   nn.MaxPool2d(2, 2))\n",
        "        #layer2\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(16, 16, 3, padding=1),\n",
        "                                   nn.BatchNorm2d(16),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(0.1),\n",
        "                                   nn.Conv2d(16, 16, 3, padding=1),\n",
        "                                   nn.BatchNorm2d(16),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(0.1),\n",
        "                                   nn.Conv2d(16, 16, 3, padding=1),\n",
        "                                   nn.BatchNorm2d(16),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(0.1),\n",
        "                                   nn.MaxPool2d(2, 2))\n",
        "        #layer2\n",
        "        self.conv3 =  nn.Sequential(nn.Conv2d(16, 16, 3),\n",
        "                                    nn.BatchNorm2d(16),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.1),\n",
        "                                    nn.Conv2d(16, 16, 3),\n",
        "                                    nn.BatchNorm2d(16),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(16, 32, 3))\n",
        "        \n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(32, 10) #This layer corresponds mnist digit prediction\n",
        "        \n",
        "        #Output for mnist digit prediction\n",
        "        self.sm = nn.Softmax(dim=-1)\n",
        "        self.adder = nn.Sequential(  #Linear network which accepts two numbers(mnist predicted output and random number bw 0 to 9)\n",
        "            nn.Linear(20, 64), \n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Linear(64, 64), \n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Linear(64, 19) # Onehot encoded output for sum of two numbers\n",
        "        )\n",
        "       \n",
        "    def forward(self, x, inp_numbers):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2(output)\n",
        "        output = self.conv3(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        \n",
        "        pred_digit = self.fc(output) #This layer outputs the predicted digit for mnist image\n",
        "        \n",
        "        pred_digit_softmaxed = self.sm(pred_digit)\n",
        "        pred_sum = self.adder(torch.cat([pred_digit_softmaxed, F.one_hot(inp_numbers, num_classes=10)], dim=-1)) #Concatenate one hot encoded mnist digit & random rumber\n",
        "\n",
        "        return pred_digit, pred_sum\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlkZLh3Gwwur",
        "outputId": "42472d42-399b-4981-8954-f67d59c501c0"
      },
      "source": [
        "print(Net())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.1, inplace=False)\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.1, inplace=False)\n",
            "    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.1, inplace=False)\n",
            "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  )\n",
            "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (sm): Softmax(dim=-1)\n",
            "  (adder): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=64, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=64, out_features=19, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M993lUV9xAc_"
      },
      "source": [
        "#**Dataset Class**\n",
        "Lets create MNISTWrapper dataset class that should override 3 methods:  \n",
        "1. **\\__init__()**: Initialization like reading image paths\n",
        "2. **\\__len__()**: Should return the number of items\n",
        "3. **\\__getitem__(i)**: Should return $i^{th}$ data-point\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ICZHVbQemgQ"
      },
      "source": [
        "class MNISTWrapper(Dataset):\n",
        "    def __init__(self, dataset, tfms):\n",
        "        self.dataset = dataset\n",
        "        self.tfms = tfms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "        rand_number = torch.randint(0, 10, (1,1)).long()\n",
        "        actual_sum = label + rand_number\n",
        "        return img, label, rand_number.item(), actual_sum.item()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIyWxgDwxyoQ"
      },
      "source": [
        "# **Data Transformations and Test/Train DataLoaders**\n",
        "A Dataloader is a wrapper around a Dataset that puts images in a batch and lets us iterate over the batches. A Dataloader can be created by just passing our dataset to its contructor.\n",
        "\n",
        "Additionally, it takes the following arguments:\n",
        "\n",
        "batch_size: How many data-points to put together in a batch\n",
        "shuffle: Whether to randomly shuffle the data order. Its important that we set this to True for the training set, and False for the test set.\n",
        "num_workers: How many processes to spawn in loading the data. This can become a bottleneck if your GPU is fast enough. Set this to the number of cores / processors available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpR-34CoemgR"
      },
      "source": [
        "def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "    #download mnist train dataset\n",
        "    mnist_training = torchvision.datasets.MNIST(\n",
        "        root='./mnist_data', download=True, transform=transform_train, train=True)\n",
        "    mnist_training = MNISTWrapper(mnist_training, transform_train)\n",
        "    #create dataloader\n",
        "    mnist_training_loader = DataLoader(\n",
        "        mnist_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    return mnist_training_loader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX-EUKqeemgR"
      },
      "source": [
        "def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "     #download mnist test dataset\n",
        "    mnist_test = torchvision.datasets.MNIST(\n",
        "        root='./mnist_data', download=True, transform=transform_test, train=False)\n",
        "    mnist_test = MNISTWrapper(mnist_test, transform_test)\n",
        "    #create dataloader\n",
        "    mnist_test_loader = DataLoader(\n",
        "        mnist_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    return mnist_test_loader"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FS4LhQ1ybcb"
      },
      "source": [
        "#**Train function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCRKZrB6emgS"
      },
      "source": [
        "def train(net, optimizer, loss_function, mnist_training_loader, devices, epoch):\n",
        "    net.train()\n",
        "    for batch_index, (images, labels, rand_numbers, actual_sums) in enumerate(mnist_training_loader):\n",
        "        #copy the data to cuda\n",
        "        labels = labels.to(f\"cuda:{devices[0]}\")\n",
        "        images = images.to(f\"cuda:{devices[0]}\")\n",
        "        rand_numbers = rand_numbers.to(f\"cuda:{devices[0]}\")\n",
        "        actual_sums = actual_sums.to(f\"cuda:{devices[0]}\")\n",
        "\n",
        "        # Init\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #predict\n",
        "        pred_digits, pred_sums = net(images, rand_numbers)\n",
        "\n",
        "        # calculate loss for digit prediction\n",
        "        loss_pred = loss_function(pred_digits, labels).mean()\n",
        "\n",
        "        # calculate loss for sum of two numbers\n",
        "        loss_sum = loss_function(pred_sums, actual_sums).mean()\n",
        "        \n",
        "        #final loss calculated by adding prediction loss & sum loss\n",
        "        loss = loss_pred + loss_sum\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_index % 10 == 0:\n",
        "            print('Training Epoch: {epoch} [{batch_index}/{total_batches}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
        "                loss.item(),\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                epoch=epoch,\n",
        "                batch_index=batch_index,\n",
        "                total_batches=len(mnist_training_loader)\n",
        "            ))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqEbGriHzlPp"
      },
      "source": [
        "# **Model evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr4_H1N1emgT"
      },
      "source": [
        "@torch.no_grad()\n",
        "def eval_training(net, loss_function, mnist_test_loader, devices, epoch=0, tb=True):\n",
        "    net.eval()\n",
        "\n",
        "    test_loss = 0.0  # cost function error\n",
        "    correct_digits = 0.0\n",
        "    correct_sums = 0.0\n",
        "\n",
        "    for (images, labels, rand_numbers, actual_sums) in mnist_test_loader:\n",
        "\n",
        "        images = images.to(f\"cuda:{devices[0]}\")\n",
        "        labels = labels.to(f\"cuda:{devices[0]}\")\n",
        "        rand_numbers = rand_numbers.to(f\"cuda:{devices[0]}\")\n",
        "        actual_sums = actual_sums.to(f\"cuda:{devices[0]}\")\n",
        "\n",
        "        pred_digits, pred_sums = net(images, rand_numbers)\n",
        "        loss_pred = loss_function(pred_digits, labels).mean()\n",
        "        loss_sum = loss_function(pred_sums, actual_sums).mean()\n",
        "        loss = loss_pred + loss_sum\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, digit_preds = pred_digits.max(1)\n",
        "        correct_digits += digit_preds.eq(labels).sum()\n",
        "        _, sum_preds = pred_sums.max(1)\n",
        "        correct_sums += sum_preds.eq(actual_sums).sum()\n",
        "\n",
        "    print('Evaluating Network.....')\n",
        "    print('Test set: Epoch: {}, Average loss: {:.4f}, Digit Accuracy: {:.4f}, Sum Accuracy: {:.4f}'.format(\n",
        "        epoch,\n",
        "        test_loss / len(mnist_test_loader.dataset),\n",
        "        correct_digits.float() / len(mnist_test_loader.dataset),\n",
        "        correct_sums.float() / len(mnist_test_loader.dataset),\n",
        "    ))\n",
        "    print()\n",
        "\n",
        "    return ((correct_digits.float() + correct_sums.float()) / 2) / len(mnist_test_loader.dataset)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlM92mvyy8Nw"
      },
      "source": [
        "#**Data transformations on training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXo6IfDuemgT",
        "outputId": "708c97ef-2133-43c2-a262-2e82501c31ee"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307), (0.3081))\n",
        "])\n",
        "mnist_training = torchvision.datasets.MNIST(\n",
        "    root='./mnist_data', download=True, transform=transform_train, train=True)\n",
        "mnist_training = MNISTWrapper(mnist_training, transform_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qvCj4u8zEiu"
      },
      "source": [
        "# **Lets set the device and train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lNlxoRxemgU",
        "outputId": "860163b1-91f2-4f7a-cdec-52e3a5104a25"
      },
      "source": [
        "devices = [0, 1]\n",
        "net = Net().to(f\"cuda:{devices[0]}\")\n",
        "\n",
        "#train dataloader\n",
        "mnist_training_loader = get_training_dataloader(\n",
        "    (0.1307),\n",
        "    (0.3081),\n",
        "    num_workers=4,\n",
        "    batch_size=1024,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "#test dataloader\n",
        "mnist_test_loader = get_test_dataloader(\n",
        "    (0.1307),\n",
        "    (0.3081),\n",
        "    num_workers=4,\n",
        "    batch_size=1024,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "#calculate loss \n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-2,\n",
        "                        momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "best_acc = 0\n",
        "for epoch in range(1, 20 + 1):\n",
        "    train(net, optimizer, loss_function, mnist_training_loader, devices, epoch)\n",
        "    acc = eval_training(net, loss_function, mnist_test_loader, devices, epoch)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch: 1 [0/59]\tLoss: 5.2718\tLR: 0.010000\n",
            "Training Epoch: 1 [10/59]\tLoss: 5.0588\tLR: 0.010000\n",
            "Training Epoch: 1 [20/59]\tLoss: 4.6491\tLR: 0.010000\n",
            "Training Epoch: 1 [30/59]\tLoss: 4.1022\tLR: 0.010000\n",
            "Training Epoch: 1 [40/59]\tLoss: 3.7026\tLR: 0.010000\n",
            "Training Epoch: 1 [50/59]\tLoss: 3.4171\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 1, Average loss: 0.0033, Digit Accuracy: 0.9129, Sum Accuracy: 0.0981\n",
            "\n",
            "Training Epoch: 2 [0/59]\tLoss: 3.2545\tLR: 0.010000\n",
            "Training Epoch: 2 [10/59]\tLoss: 3.1639\tLR: 0.010000\n",
            "Training Epoch: 2 [20/59]\tLoss: 3.1006\tLR: 0.010000\n",
            "Training Epoch: 2 [30/59]\tLoss: 3.0736\tLR: 0.010000\n",
            "Training Epoch: 2 [40/59]\tLoss: 3.0388\tLR: 0.010000\n",
            "Training Epoch: 2 [50/59]\tLoss: 3.0358\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 2, Average loss: 0.0030, Digit Accuracy: 0.9501, Sum Accuracy: 0.0985\n",
            "\n",
            "Training Epoch: 3 [0/59]\tLoss: 2.9929\tLR: 0.010000\n",
            "Training Epoch: 3 [10/59]\tLoss: 2.9617\tLR: 0.010000\n",
            "Training Epoch: 3 [20/59]\tLoss: 2.9392\tLR: 0.010000\n",
            "Training Epoch: 3 [30/59]\tLoss: 2.9451\tLR: 0.010000\n",
            "Training Epoch: 3 [40/59]\tLoss: 2.9159\tLR: 0.010000\n",
            "Training Epoch: 3 [50/59]\tLoss: 2.9350\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 3, Average loss: 0.0029, Digit Accuracy: 0.9757, Sum Accuracy: 0.1033\n",
            "\n",
            "Training Epoch: 4 [0/59]\tLoss: 2.9123\tLR: 0.010000\n",
            "Training Epoch: 4 [10/59]\tLoss: 2.8741\tLR: 0.010000\n",
            "Training Epoch: 4 [20/59]\tLoss: 2.8998\tLR: 0.010000\n",
            "Training Epoch: 4 [30/59]\tLoss: 2.8998\tLR: 0.010000\n",
            "Training Epoch: 4 [40/59]\tLoss: 2.8488\tLR: 0.010000\n",
            "Training Epoch: 4 [50/59]\tLoss: 2.8313\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 4, Average loss: 0.0028, Digit Accuracy: 0.9782, Sum Accuracy: 0.0999\n",
            "\n",
            "Training Epoch: 5 [0/59]\tLoss: 2.8716\tLR: 0.010000\n",
            "Training Epoch: 5 [10/59]\tLoss: 2.8457\tLR: 0.010000\n",
            "Training Epoch: 5 [20/59]\tLoss: 2.8673\tLR: 0.010000\n",
            "Training Epoch: 5 [30/59]\tLoss: 2.8415\tLR: 0.010000\n",
            "Training Epoch: 5 [40/59]\tLoss: 2.8385\tLR: 0.010000\n",
            "Training Epoch: 5 [50/59]\tLoss: 2.8405\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 5, Average loss: 0.0028, Digit Accuracy: 0.9815, Sum Accuracy: 0.0991\n",
            "\n",
            "Training Epoch: 6 [0/59]\tLoss: 2.8075\tLR: 0.010000\n",
            "Training Epoch: 6 [10/59]\tLoss: 2.7998\tLR: 0.010000\n",
            "Training Epoch: 6 [20/59]\tLoss: 2.8023\tLR: 0.010000\n",
            "Training Epoch: 6 [30/59]\tLoss: 2.8118\tLR: 0.010000\n",
            "Training Epoch: 6 [40/59]\tLoss: 2.8115\tLR: 0.010000\n",
            "Training Epoch: 6 [50/59]\tLoss: 2.7761\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 6, Average loss: 0.0027, Digit Accuracy: 0.9845, Sum Accuracy: 0.0990\n",
            "\n",
            "Training Epoch: 7 [0/59]\tLoss: 2.7446\tLR: 0.010000\n",
            "Training Epoch: 7 [10/59]\tLoss: 2.7354\tLR: 0.010000\n",
            "Training Epoch: 7 [20/59]\tLoss: 2.7439\tLR: 0.010000\n",
            "Training Epoch: 7 [30/59]\tLoss: 2.7250\tLR: 0.010000\n",
            "Training Epoch: 7 [40/59]\tLoss: 2.6828\tLR: 0.010000\n",
            "Training Epoch: 7 [50/59]\tLoss: 2.7061\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 7, Average loss: 0.0026, Digit Accuracy: 0.9859, Sum Accuracy: 0.1437\n",
            "\n",
            "Training Epoch: 8 [0/59]\tLoss: 2.6718\tLR: 0.010000\n",
            "Training Epoch: 8 [10/59]\tLoss: 2.6158\tLR: 0.010000\n",
            "Training Epoch: 8 [20/59]\tLoss: 2.5985\tLR: 0.010000\n",
            "Training Epoch: 8 [30/59]\tLoss: 2.5859\tLR: 0.010000\n",
            "Training Epoch: 8 [40/59]\tLoss: 2.5720\tLR: 0.010000\n",
            "Training Epoch: 8 [50/59]\tLoss: 2.5261\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 8, Average loss: 0.0025, Digit Accuracy: 0.9885, Sum Accuracy: 0.2216\n",
            "\n",
            "Training Epoch: 9 [0/59]\tLoss: 2.5084\tLR: 0.010000\n",
            "Training Epoch: 9 [10/59]\tLoss: 2.4333\tLR: 0.010000\n",
            "Training Epoch: 9 [20/59]\tLoss: 2.3897\tLR: 0.010000\n",
            "Training Epoch: 9 [30/59]\tLoss: 2.3802\tLR: 0.010000\n",
            "Training Epoch: 9 [40/59]\tLoss: 2.3239\tLR: 0.010000\n",
            "Training Epoch: 9 [50/59]\tLoss: 2.2761\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 9, Average loss: 0.0022, Digit Accuracy: 0.9854, Sum Accuracy: 0.3605\n",
            "\n",
            "Training Epoch: 10 [0/59]\tLoss: 2.2303\tLR: 0.010000\n",
            "Training Epoch: 10 [10/59]\tLoss: 2.1972\tLR: 0.010000\n",
            "Training Epoch: 10 [20/59]\tLoss: 2.1696\tLR: 0.010000\n",
            "Training Epoch: 10 [30/59]\tLoss: 2.0980\tLR: 0.010000\n",
            "Training Epoch: 10 [40/59]\tLoss: 2.1071\tLR: 0.010000\n",
            "Training Epoch: 10 [50/59]\tLoss: 2.0483\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 10, Average loss: 0.0020, Digit Accuracy: 0.9902, Sum Accuracy: 0.4110\n",
            "\n",
            "Training Epoch: 11 [0/59]\tLoss: 2.0070\tLR: 0.010000\n",
            "Training Epoch: 11 [10/59]\tLoss: 1.9880\tLR: 0.010000\n",
            "Training Epoch: 11 [20/59]\tLoss: 1.9384\tLR: 0.010000\n",
            "Training Epoch: 11 [30/59]\tLoss: 1.8808\tLR: 0.010000\n",
            "Training Epoch: 11 [40/59]\tLoss: 1.9044\tLR: 0.010000\n",
            "Training Epoch: 11 [50/59]\tLoss: 1.8214\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 11, Average loss: 0.0018, Digit Accuracy: 0.9900, Sum Accuracy: 0.5553\n",
            "\n",
            "Training Epoch: 12 [0/59]\tLoss: 1.8179\tLR: 0.010000\n",
            "Training Epoch: 12 [10/59]\tLoss: 1.7585\tLR: 0.010000\n",
            "Training Epoch: 12 [20/59]\tLoss: 1.7466\tLR: 0.010000\n",
            "Training Epoch: 12 [30/59]\tLoss: 1.7025\tLR: 0.010000\n",
            "Training Epoch: 12 [40/59]\tLoss: 1.6444\tLR: 0.010000\n",
            "Training Epoch: 12 [50/59]\tLoss: 1.6498\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 12, Average loss: 0.0016, Digit Accuracy: 0.9907, Sum Accuracy: 0.5490\n",
            "\n",
            "Training Epoch: 13 [0/59]\tLoss: 1.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [10/59]\tLoss: 1.5918\tLR: 0.010000\n",
            "Training Epoch: 13 [20/59]\tLoss: 1.5769\tLR: 0.010000\n",
            "Training Epoch: 13 [30/59]\tLoss: 1.5404\tLR: 0.010000\n",
            "Training Epoch: 13 [40/59]\tLoss: 1.5444\tLR: 0.010000\n",
            "Training Epoch: 13 [50/59]\tLoss: 1.5000\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 13, Average loss: 0.0014, Digit Accuracy: 0.9898, Sum Accuracy: 0.6692\n",
            "\n",
            "Training Epoch: 14 [0/59]\tLoss: 1.4599\tLR: 0.010000\n",
            "Training Epoch: 14 [10/59]\tLoss: 1.4546\tLR: 0.010000\n",
            "Training Epoch: 14 [20/59]\tLoss: 1.4094\tLR: 0.010000\n",
            "Training Epoch: 14 [30/59]\tLoss: 1.3780\tLR: 0.010000\n",
            "Training Epoch: 14 [40/59]\tLoss: 1.4038\tLR: 0.010000\n",
            "Training Epoch: 14 [50/59]\tLoss: 1.3300\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 14, Average loss: 0.0013, Digit Accuracy: 0.9901, Sum Accuracy: 0.7566\n",
            "\n",
            "Training Epoch: 15 [0/59]\tLoss: 1.3446\tLR: 0.010000\n",
            "Training Epoch: 15 [10/59]\tLoss: 1.2909\tLR: 0.010000\n",
            "Training Epoch: 15 [20/59]\tLoss: 1.3028\tLR: 0.010000\n",
            "Training Epoch: 15 [30/59]\tLoss: 1.2549\tLR: 0.010000\n",
            "Training Epoch: 15 [40/59]\tLoss: 1.2276\tLR: 0.010000\n",
            "Training Epoch: 15 [50/59]\tLoss: 1.2246\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 15, Average loss: 0.0012, Digit Accuracy: 0.9908, Sum Accuracy: 0.8420\n",
            "\n",
            "Training Epoch: 16 [0/59]\tLoss: 1.1972\tLR: 0.010000\n",
            "Training Epoch: 16 [10/59]\tLoss: 1.2107\tLR: 0.010000\n",
            "Training Epoch: 16 [20/59]\tLoss: 1.1799\tLR: 0.010000\n",
            "Training Epoch: 16 [30/59]\tLoss: 1.1183\tLR: 0.010000\n",
            "Training Epoch: 16 [40/59]\tLoss: 1.1215\tLR: 0.010000\n",
            "Training Epoch: 16 [50/59]\tLoss: 1.1134\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 16, Average loss: 0.0011, Digit Accuracy: 0.9919, Sum Accuracy: 0.9192\n",
            "\n",
            "Training Epoch: 17 [0/59]\tLoss: 1.1059\tLR: 0.010000\n",
            "Training Epoch: 17 [10/59]\tLoss: 1.0496\tLR: 0.010000\n",
            "Training Epoch: 17 [20/59]\tLoss: 1.0558\tLR: 0.010000\n",
            "Training Epoch: 17 [30/59]\tLoss: 1.0330\tLR: 0.010000\n",
            "Training Epoch: 17 [40/59]\tLoss: 1.0399\tLR: 0.010000\n",
            "Training Epoch: 17 [50/59]\tLoss: 1.0066\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 17, Average loss: 0.0010, Digit Accuracy: 0.9909, Sum Accuracy: 0.9596\n",
            "\n",
            "Training Epoch: 18 [0/59]\tLoss: 0.9712\tLR: 0.010000\n",
            "Training Epoch: 18 [10/59]\tLoss: 1.0433\tLR: 0.010000\n",
            "Training Epoch: 18 [20/59]\tLoss: 0.9864\tLR: 0.010000\n",
            "Training Epoch: 18 [30/59]\tLoss: 0.9565\tLR: 0.010000\n",
            "Training Epoch: 18 [40/59]\tLoss: 0.9560\tLR: 0.010000\n",
            "Training Epoch: 18 [50/59]\tLoss: 0.9275\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 18, Average loss: 0.0009, Digit Accuracy: 0.9908, Sum Accuracy: 0.9563\n",
            "\n",
            "Training Epoch: 19 [0/59]\tLoss: 0.9788\tLR: 0.010000\n",
            "Training Epoch: 19 [10/59]\tLoss: 0.8501\tLR: 0.010000\n",
            "Training Epoch: 19 [20/59]\tLoss: 0.8521\tLR: 0.010000\n",
            "Training Epoch: 19 [30/59]\tLoss: 0.8473\tLR: 0.010000\n",
            "Training Epoch: 19 [40/59]\tLoss: 0.8943\tLR: 0.010000\n",
            "Training Epoch: 19 [50/59]\tLoss: 0.8304\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 19, Average loss: 0.0008, Digit Accuracy: 0.9924, Sum Accuracy: 0.9604\n",
            "\n",
            "Training Epoch: 20 [0/59]\tLoss: 0.8368\tLR: 0.010000\n",
            "Training Epoch: 20 [10/59]\tLoss: 0.8270\tLR: 0.010000\n",
            "Training Epoch: 20 [20/59]\tLoss: 0.8662\tLR: 0.010000\n",
            "Training Epoch: 20 [30/59]\tLoss: 0.8038\tLR: 0.010000\n",
            "Training Epoch: 20 [40/59]\tLoss: 0.8204\tLR: 0.010000\n",
            "Training Epoch: 20 [50/59]\tLoss: 0.7875\tLR: 0.010000\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 20, Average loss: 0.0007, Digit Accuracy: 0.9918, Sum Accuracy: 0.9624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0C_TWVY0OQe"
      },
      "source": [
        "# **Test data transformation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaMUh6GqemgW"
      },
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307),(0.3081))\n",
        "])\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root='./mnist_data', download=True, transform=transform_test, train=False)\n",
        "mnist_test = MNISTWrapper(mnist_test, transform_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqhpnFf_0SIB"
      },
      "source": [
        "#**Let's test the model for digit and sum prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "pLYE5zX_emgX",
        "outputId": "56129785-086a-49c4-db17-ad011d899e9f"
      },
      "source": [
        "image, label, rand_number, actual_sum = mnist_test[1000]\n",
        "print(f\"Inputs:\\nRandom number: {rand_number}\")\n",
        "plt.matshow(image.numpy()[0])\n",
        "pred_digit, pred_sum = net(image.unsqueeze(0).to(f\"cuda:{devices[0]}\"), torch.LongTensor([rand_number]).to(f\"cuda:{devices[0]}\"))\n",
        "_, pred_digit = pred_digit.max(1)\n",
        "_, pred_sum = pred_sum.max(1)\n",
        "print(f\"Outputs:\\nPredicted digit: {pred_digit.item()}\\nPredicted sum: {pred_sum.item()}\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "Random number: 3\n",
            "Outputs:\n",
            "Predicted digit: 9\n",
            "Predicted sum: 12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "D99QPL9MemgX",
        "outputId": "1c35033e-254c-4e54-bd19-f7c56fa72129"
      },
      "source": [
        "#get random data\n",
        "image, label, rand_number, actual_sum = mnist_test[2000]\n",
        "print(f\"Inputs:\\nRandom number: {rand_number}\")\n",
        "plt.matshow(image.numpy()[0])\n",
        "#unsqueeze input and copy data to cuda \n",
        "pred_digit, pred_sum = net(image.unsqueeze(0).to(f\"cuda:{devices[0]}\"), torch.LongTensor([rand_number]).to(f\"cuda:{devices[0]}\"))\n",
        "_, pred_digit = pred_digit.max(1)\n",
        "_, pred_sum = pred_sum.max(1)\n",
        "print(f\"Outputs:\\nPredicted digit: {pred_digit.item()}\\nPredicted sum: {pred_sum.item()}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "Random number: 8\n",
            "Outputs:\n",
            "Predicted digit: 6\n",
            "Predicted sum: 14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "w21N6kxeemgY",
        "outputId": "c6e28a5d-d76e-465c-f894-e8bd94bb18c3"
      },
      "source": [
        "image, label, rand_number, actual_sum = mnist_test[3000]\n",
        "print(f\"Inputs:\\nRandom number: {rand_number}\")\n",
        "plt.matshow(image.numpy()[0])\n",
        "pred_digit, pred_sum = net(image.unsqueeze(0).to(f\"cuda:{devices[0]}\"), torch.LongTensor([rand_number]).to(f\"cuda:{devices[0]}\"))\n",
        "_, pred_digit = pred_digit.max(1)\n",
        "_, pred_sum = pred_sum.max(1)\n",
        "print(f\"Outputs:\\nPredicted digit: {pred_digit.item()}\\nPredicted sum: {pred_sum.item()}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "Random number: 7\n",
            "Outputs:\n",
            "Predicted digit: 6\n",
            "Predicted sum: 13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGSjP9c-emgY"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}